{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy and SciSpacy \n",
    "\n",
    "[Spacy](https://spacy.io/usage/processing-pipelines) is one of the main NLP open-source libraries and contributors.\n",
    "\n",
    "They have a vast amount of methods, pipelines, pre-trained models, tools and tutorials for NLP. Anything from cleaning, linguistic analysis and advanced modelling capabilties are available in Spacy. \n",
    "\n",
    "One of their most interesting capabilities is found on their related SciSpacy module, that brings several biomedical models and tools for Medical NLP analysis. \n",
    "\n",
    "This notebooks covers a few of the basic topics in the Spacy tool-kit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import spacy\n",
    "from scispacy.linking import EntityLinker\n",
    "\n",
    "# Load the model\n",
    "# warning make sure you download the relevant model first.\n",
    "# check README for instructions.\n",
    "nlp = spacy.load(\"en_core_sci_md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text: str = \"Alterations in the hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in people.\"\n",
    "# add multiple NLP based attributes and methods to the text\n",
    "doc = nlp(text)\n",
    "# For example we now have the options to\n",
    "# identify if a words is a stopword or not.\n",
    "# use spacy's lemmatizer, get the Part of Speech of the word.\n",
    "# as well as any dependency\n",
    "for token in doc:\n",
    "    if not token.is_stop:\n",
    "        print(\n",
    "            token.text, \"->\",\n",
    "            token.lemma_,  # lemmatizer\n",
    "            token.pos_,  # Part of Speech\n",
    "            token.tag_,  # Tag \n",
    "            token.dep_,  # Dependencies\n",
    "            token.shape_  # Shape -> Caps, Case representation\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(next(doc.sents), style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Linker\n",
    "\n",
    "One of the most interesting features of SciSpacy is the entity-linker, which is better described in this [link](https://github.com/allenai/scispacy?tab=readme-ov-file#entitylinker:~:text=config%3D%7B%22make_serializable%22%3A%20True%7D-,EntityLinker,-The%20EntityLinker%20is)\n",
    "\n",
    "It basically allows you to relate an entity to a particular knowledge base. For example, Ibuprofen is linked to the drug base and you can then get further details of the linked entity.\n",
    "\n",
    "** WARNING ** This will download 1GB of data and it's slow at the beginning (then it should be faster as it is cached). However, adding this to your pipeline will severely decrease processing speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line takes a while, because we have to download ~1GB of data\n",
    "# and load a large JSON file (the knowledge base). Be patient!\n",
    "# Thankfully it should be faster after the first time you use it, because\n",
    "# the downloads are cached.\n",
    "# NOTE: The resolve_abbreviations parameter is optional, and requires that\n",
    "# the AbbreviationDetector pipe has already been added to the pipeline. Adding\n",
    "# the AbbreviationDetector pipe and setting resolve_abbreviations to True means\n",
    "# that linking will only be performed on the long form of abbreviations.\n",
    "\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\n",
    "    \"Spinal and bulbar muscular atrophy (SBMA) is an \\\n",
    "    inherited motor neuron disease caused by the expansion \\\n",
    "    of a polyglutamine tract within the androgen receptor (AR). \\\n",
    "    SBMA can be caused by this easily.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = doc.ents[2]\n",
    "\n",
    "print(\"Name: \", entity)\n",
    "# Each entity is linked to UMLS with a score\n",
    "# (currently just char-3gram matching).\n",
    "linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "for umls_ent in entity._.kb_ents:\n",
    "\tprint(linker.kb.cui_to_entity[umls_ent[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word and text similarity\n",
    "\n",
    "Text similarity has been primordial in NLP since its beginning and even with the great advancements in GenAI, the methods used to semantically compare words and sentences mainly rely in vector operations and \"distance\" metrics. For example, we know that `shark` and `whale` are more closely related to each other than `shark` and `computer`. Likewise, with modern language models (Word2Vec, transformers, etc.) we can mathematically represent this. Usually, using a common distance metric like euclidean distance or cosine similarity. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compare words\n",
    "text1 = \"shark\"\n",
    "text2 = \"whale\"\n",
    "text3 = \"computer\"\n",
    "\n",
    "print(\n",
    "    \"Similarity Shark and Whale:\",\n",
    "    nlp(text1).similarity(nlp(text2))\n",
    ")\n",
    "print()\n",
    "print(\n",
    "    \"Similarity Shark and Computer:\",\n",
    "    nlp(text1).similarity(nlp(text3))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can do the same with senteces \n",
    "# Spacy first calculates the mean of the sentence\n",
    "# vector to compare across sentences. \n",
    "text1 = \"Tylenol is used to treat headaches\"\n",
    "text2 = \"Ibuprofen is used to alleviate migraines\"\n",
    "\n",
    "nlp(text1).similarity(nlp(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A completely unrelated sentece.\n",
    "text3 = \"There is no place like home\"\n",
    "\n",
    "nlp(text1).similarity(nlp(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Vector representation \n",
    "word: str = \"melanoma\"\n",
    "word_id = nlp.vocab.strings[word]\n",
    "word_vector = nlp.vocab.vectors[word_id]\n",
    "print(word_vector[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also get the vector of a sentece \n",
    "\n",
    "nlp(\"Hello, this is a sentece\").vector[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic similarity example\n",
    "\n",
    "In NLP, one of the biggest breakthroughs came when the Word2Vec model was able \"answer\" to the famous analogy riddle. `King` is to `man` as `woman` is to ____. For humans, it's easy to find the analogy and responde correctly `queen`, however this was thought to be almost impossible for an AI. In a vector representation, these could be noted as `x = king - man + woman`. When using the original Word2Vec model, the resulting vector will be very close to the vector for `queen`, hence adding a whole new dimmention to NLP and AI. \n",
    "\n",
    "We can do something similar with the bio-medical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "king = nlp(\"cardiologist\").vector\n",
    "man = nlp(\"heart\").vector\n",
    "woman = nlp(\"brain\").vector\n",
    "\n",
    "result = king - man + woman\n",
    "\n",
    "# Format the vocabulary for use in the distance function\n",
    "ids = [x for x in nlp.vocab.vectors.keys()]\n",
    "vectors = [nlp.vocab.vectors[x] for x in ids]\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "\n",
    "# *** Find the closest word below ***\n",
    "closest_index = distance.cdist(np.array(result.reshape(1,-1)), vectors).argmin()\n",
    "word_id = ids[closest_index]\n",
    "output_word = nlp.vocab[word_id].text\n",
    "output_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy's Pipelines\n",
    "\n",
    "However, I would say Spacy's greatest feature is its capacity to create a Pipe (pipeline) with multiple transformations. It allows you to set up an elaborate pre-processing pipeline to efficiently clean, tag and analyse your text input. For example, let's create a cleaning pipeline, in which we can remove some of the attributes and models to make it run faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from time import time  # Medir tiempo de ejecucion\n",
    "\n",
    "\n",
    "data_path : str = \"../data/mtsamples.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "# let's pick the first 50 transcriptions as example \n",
    "transcriptions = df[\"transcription\"].dropna()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the NLP model.\n",
    "nlp = spacy.load(\n",
    "  'en_core_sci_md',\n",
    "  disable=['ner', 'parser']  # let's remove some things we don't need for cleaning\n",
    ") \n",
    "\n",
    "def cleaning(doc) -> str:\n",
    "    \"\"\"Simple cleaning pipeline using Spacy.\n",
    "\n",
    "    Lemmatize and eliminates stopwords. Keeps only alpha (removes digits)\n",
    "    Args:\n",
    "        doc (spacy.tokens.doc.Doc): Document processed by spacy's pipeline\n",
    "    Returns:\n",
    "        str: Processed String.\n",
    "    \"\"\"\n",
    "    txt = [\n",
    "      token.lemma_ for token in doc if not token.is_stop and token.is_alpha\n",
    "    ]\n",
    "    return ' '.join(txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example in a short sentence\n",
    "text: str = \"Alterations in the Hypocretin receptor 2 and preprohypocretin genes produce narcolepsy in people.\"\n",
    "cleaning(nlp(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time() # let's measure execution time\n",
    "\n",
    "txt = [\n",
    "    cleaning(doc) for doc in nlp.pipe(\n",
    "        transcriptions,\n",
    "        batch_size=20,\n",
    "        n_process=1  # number of processors to use. \n",
    "    )\n",
    "]\n",
    "# medimos tiempo de ejecución \n",
    "t_ = round((time() - t) / 60, 2)  # seconds needed to run\n",
    "print(f'Execution time: {t_} mins')\n",
    "print(txt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without multiprocessing \n",
    "\n",
    "t = time() # let's measure execution time\n",
    "\n",
    "txt = [\n",
    "    cleaning(nlp(doc)) for doc in transcriptions\n",
    "]\n",
    "# medimos tiempo de ejecución \n",
    "t_ = round((time() - t) / 60, 2)  # seconds needed to run\n",
    "print(f'Execution time: {t_} mins')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
