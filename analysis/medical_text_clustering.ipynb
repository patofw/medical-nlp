{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Text Clustering\n",
    "\n",
    "The MTSamples Dataset has medical transcriptions from several conditions and consultations. It also provides a medical speciality and keywords extracted manually from the transcription. The information however, is not very specific. For example, all of the `surgery` consultations are under the same `medical_speciality` field. Nonetheless, \"surgery\" is a very broad term, and it can go from minor molar procedures to life-or-death cardiovascular operations. \n",
    "\n",
    "The task is to run a clustering algorithm that is able to provide sub-groups within the surgery speciality. The end-goal is to have sub-topics clearly identified under the surgery category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "# load from our own module\n",
    "# Follow installation in the Readme beforehand!\n",
    "from medical_nlp import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up\n",
    "\n",
    "# Load the model!\n",
    "# For vectors, at least a size MD model is needed!\n",
    "nlp = spacy.load(\"en_core_sci_md\")\n",
    "\n",
    "txt_col : str = \"transcription\"\n",
    "data_path : str = \"../data/mtsamples.csv\"\n",
    "\n",
    "# You can play with the params and see different results!\n",
    "MODEL_PARAMS: dict = {\n",
    "    \"eps\": 0.25,\n",
    "    \"min_samples\": 12\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "# drop nas from the text column\n",
    "df = df[df[txt_col].notna()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top ten specialities\n",
    "df.medical_specialty.value_counts()[:10].plot(kind=\"bar\", title=\"Top Ten medical specialities in the data\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering for only Surgery Speciality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only Surgery topics \n",
    "df = df[df[\"medical_specialty\"] == \" Surgery\"].reset_index(drop=True)\n",
    "# process the keywords as it will be easier to handle as a list of lists\n",
    "df[\"processed_kw\"] = df.keywords.apply(lambda row: utils.process_keywords(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "To make our clusters, we need to first have our text inputs in a vectorial representation. This will allow us to use traditional unsupervised machine learning methods to get our clusters.\n",
    "Transforming text to clusters can be done in multiple ways (from basic frequency counts to the state-of-the-art deep learning Transformer models), however for this tutorial we will use our SciSpacy model to get the vectors we need. \n",
    "\n",
    "SciSpacy (and Spacy in general) makes it really simple as they have already developed execellent methods to vectorize and get text similarities in their core library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector representation of the transcription\n",
    "# Takes around 5 minutes!\n",
    "df[\"vectors\"] = df[txt_col].apply(\n",
    "    lambda sent : utils.get_vectors(sent, nlp)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Clustering Model \n",
    "\n",
    "We are going to use the DBSCAN unsupervised algorithm to build our clusters. This algorithm is particulary useful in finding non-linear relationships, based on densities. It does not require the user to \"pre-define\" a number of clusters (contrary to K-Means for example) and it works well with high dimmensions (such as word embeddings). \n",
    "\n",
    "More info: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- You don't need to define the number of K.\n",
    "- Robust to Oultiers, since its definition \"labels\" them as such if they do not have a considerable number of neighbors (low density)\n",
    "- Excellent algorithm on arbitrary distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DBScan model\n",
    "# setting in the format DBScan requires\n",
    "embeddings = utils.format_embeddings_for_clustering(df.vectors.values)  \n",
    "# Performing DBSCAN clustering\n",
    "dbscan = DBSCAN(\n",
    "    **MODEL_PARAMS\n",
    ")\n",
    "# Get the clusters. Remember, -1 == outlier (Noise)!\n",
    "labels = dbscan.fit_predict(embeddings)\n",
    "# Add the labels back to the DF\n",
    "df[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the clustering result\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Our embeddings have 200 dimmensions, and of course, we can't plot in 200D. \n",
    "# The best we can do is 3D, so we choose a random set of dimmensions.\n",
    "d1 = 10\n",
    "d2 = 20\n",
    "d3 = 30\n",
    "# Plot points belonging to cluster -1 with reduced alpha and a different marker\n",
    "ax.scatter(\n",
    "    embeddings[dbscan.labels_ == -1, d1], \n",
    "    embeddings[dbscan.labels_ == -1, d2], \n",
    "    embeddings[dbscan.labels_ == -1, d3], \n",
    "    c='black', marker='x', alpha=0.2, label='Noise'\n",
    ")\n",
    "\n",
    "# Plot points belonging to clusters other than -1\n",
    "for label in set(dbscan.labels_) - {-1}:\n",
    "    ax.scatter(\n",
    "        embeddings[dbscan.labels_ == label, d1], \n",
    "        embeddings[dbscan.labels_ == label, d2], \n",
    "        embeddings[dbscan.labels_ == label, d3], \n",
    "        label=f'Cluster {label}'\n",
    ")\n",
    "\n",
    "ax.set_title('DBSCAN Clustering viz (3D)')\n",
    "ax.set_xlabel('Dim1')\n",
    "ax.set_ylabel('Dim2')\n",
    "ax.set_zlabel('Dim3')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Clusters\n",
    "\n",
    "Now that we have our clusters, we need to evaluate that they have been succesfully grouped together by their semantic similarity. In other words, we expect that similar topics would be clustered together.\n",
    "Let's give it a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary that will store most frequent terms\n",
    "# per label\n",
    "TOP_N : int = 10  # Top 10 terms\n",
    "freq_dict: dict = {}\n",
    "\n",
    "for label_ in df.cluster.unique():\n",
    "    if label_ != -1:  # skip noise\n",
    "        cluster_sample = df.query(\"cluster == @label_\")\n",
    "        # skip \"surgery\" (present in all) and empty keywords.\n",
    "        exclude = [\"surgery\", \"\"]\n",
    "        # count word freq\n",
    "        counts = utils.get_most_common(\n",
    "            cluster_sample.processed_kw.values,\n",
    "            top_n=TOP_N,\n",
    "            exclude=exclude\n",
    "        )\n",
    "        # assign counter to dictionary.\n",
    "        freq_dict[label_] = counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's give it a look\n",
    "for key in freq_dict:\n",
    "    print(f\"Cluster {key}\")\n",
    "    print(freq_dict[key])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our clusters. What can you see?\n",
    "\n",
    "- **Cluster 0** has to do with bone injuries, like broken feet, knee surgeries, cervical issues. \n",
    "- **Cluster 1** is clearly related to eye issues. Cataract and intraocular surgeries.\n",
    "- **Cluster 2** is related to genitals and reproductory organs mainly.\n",
    "- **Cluster 3** treats heart and cardivascular operations. \n",
    "- **Cluster 4** Mainly covers surgeries around the stomach and digestive system. \n",
    "\n",
    "The algorithm has \"learnt\" relevant patterns and has created actionable clusters! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
